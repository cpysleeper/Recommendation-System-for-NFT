{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ff6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_nft_data(api_key, collection_slug, limit=50):\n",
    "    \"\"\"Fetch NFT data from OpenSea API.\"\"\"\n",
    "    url = f\"https://api.opensea.io/api/v1/assets\"\n",
    "    headers = {\"X-API-KEY\": api_key}\n",
    "    params = {\"collection\": collection_slug, \"limit\": limit}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data: {response.json()}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json().get(\"assets\", [])\n",
    "    nft_list = [\n",
    "        {\n",
    "            \"name\": item.get(\"name\", \"Unnamed NFT\"),\n",
    "            \"description\": item.get(\"description\", \"No description available\"),\n",
    "            \"image_url\": item.get(\"image_url\", \"\"),\n",
    "            \"permalink\": item.get(\"permalink\", \"\"),\n",
    "        }\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    return nft_list\n",
    "\n",
    "# Example Usage\n",
    "api_key = \"YOUR_OPENSEA_API_KEY\"  # Replace with your OpenSea API Key\n",
    "collection_slug = \"boredapeyachtclub\"  # Example collection slug\n",
    "nft_data = get_nft_data(api_key, collection_slug)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(nft_data)\n",
    "df.to_csv(\"opensea_nft_data.csv\", index=False)\n",
    "print(\"Data saved to opensea_nft_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef635062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.opensea.io/api/v2/collections/pudgypenguins%2Foverview/stats\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": \n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Your OpenSea API Key\n",
    "\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": \n",
    "}\n",
    "\n",
    "def get_top_collections(limit=50):\n",
    "    \"\"\"Fetch top NFT collections using OpenSea API.\"\"\"\n",
    "    url = \"https://api.opensea.io/api/v2/collections/pudgypenguins%2Foverview/stats\"\n",
    "    headers = {\"accept\": \"application/json\", \"x-api-key\": API_KEY}\n",
    "    params = {\"offset\": 0, \"limit\": limit}  # Fetch top `limit` collections\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data: {response.status_code}, {response.text}\")\n",
    "        return []\n",
    "\n",
    "    collections = response.json().get(\"collections\", [])\n",
    "    nft_list = [\n",
    "        {\n",
    "            \"slug\": item.get(\"slug\", \"\"),\n",
    "            \"name\": item.get(\"name\", \"Unnamed Collection\"),\n",
    "            \"description\": item.get(\"description\", \"No description available\"),\n",
    "            \"opensea_url\": f\"https://opensea.io/collection/{item.get('slug', '')}\"\n",
    "        }\n",
    "        for item in collections\n",
    "    ]\n",
    "    \n",
    "    return nft_list\n",
    "\n",
    "get_top_collections_from_web()\n",
    "\n",
    "# def get_collection_details(slug):\n",
    "#     \"\"\"Fetch NFT collection details from OpenSea API\"\"\"\n",
    "#     url = f\"https://api.opensea.io/api/v2/collections/{slug}/overview/stats\"\n",
    "#     headers = {\"accept\": \"application/json\", \"x-api-key\": API_KEY}\n",
    "\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Error fetching collection {slug}: {response.text}\")\n",
    "#         return {}\n",
    "\n",
    "#     return response.json()\n",
    "\n",
    "\n",
    "# def crawl_opensea_collections():\n",
    "#     \"\"\"Main function to scrape OpenSea collections and save all data\"\"\"\n",
    "#     collections = get_top_collections_from_web()\n",
    "    \n",
    "#     print(f\"Found {len(collections)} collections. Fetching details...\")\n",
    "\n",
    "#     for collection in collections:\n",
    "#         time.sleep(2)  # Prevent being blocked\n",
    "#         details = get_collection_details(collection[\"slug\"])\n",
    "#         collection.update(details)  # Merge details into collection data\n",
    "#         print(f\"Fetched: {collection['name']}\")\n",
    "\n",
    "#     # Save to JSON\n",
    "#     with open(\"opensea_collections.json\", \"w\") as f:\n",
    "#         json.dump(collections, f, indent=4)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     df = pd.DataFrame(collections)\n",
    "#     df.to_csv(\"opensea_collections.csv\", index=False)\n",
    "\n",
    "#     print(\"Data saved to opensea_collections.json and opensea_collections.csv\")\n",
    "\n",
    "\n",
    "# # Run the crawler\n",
    "# crawl_opensea_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"ethereum\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"seven_day_volume\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_eth.json\"\n",
    "csv_filename = \"opensea_collections_eth.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "# API URL to fetch collections\n",
    "url = \"https://api.opensea.io/api/v2/collections?chain=matic&limit=100&order_by=seven_day_volume\"\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": \n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Convert response to JSON\n",
    "\n",
    "    # Save data as JSON file\n",
    "    with open(\"opensea_collections_matic.json\", \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "    # Convert to DataFrame for CSV export\n",
    "    collections = data.get(\"collections\", [])  # Extract collections list\n",
    "    df = pd.DataFrame(collections)\n",
    "\n",
    "    # Save data as CSV file\n",
    "    df.to_csv(\"opensea_collections_matic.csv\", index=False)\n",
    "\n",
    "    print(\"✅ Data successfully saved as 'opensea_collections.json' and 'opensea_collections.csv'.\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"matic\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"seven_day_volume\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_matic.json\"\n",
    "csv_filename = \"opensea_collections_matic.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"arbitrum\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"seven_day_volume\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_arbitrum.json\"\n",
    "csv_filename = \"opensea_collections_arbitrum.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"b3\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"seven_day_volume\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_b3.json\"\n",
    "csv_filename = \"opensea_collections_b3.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"flow\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"seven_day_volume\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_flow.json\"\n",
    "csv_filename = \"opensea_collections_flow.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"shape\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"seven_day_volume\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_shape.json\"\n",
    "csv_filename = \"opensea_collections_shape.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74353e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limits\n",
    "\n",
    "# OpenSea API Key\n",
    "\n",
    "\n",
    "# Initial API URL\n",
    "base_url = \"https://api.opensea.io/api/v2/collections\"\n",
    "params = {\n",
    "    \"chain\": \"avalanche\",\n",
    "    \"limit\": 100,\n",
    "    \"order_by\": \"created_date\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Output files\n",
    "json_filename = \"opensea_collections_avalanche.json\"\n",
    "csv_filename = \"opensea_collections_avalanche.csv\"\n",
    "\n",
    "# Initialize an empty DataFrame for appending data\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "next_token = None  # Pagination token\n",
    "page_count = 0  # Counter for saved pages\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params[\"next\"] = next_token  # Add pagination token if available\n",
    "\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Convert response to JSON\n",
    "\n",
    "        # Save JSON data (append each batch separately)\n",
    "        with open(json_filename, \"a\") as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")  # New line for readability\n",
    "\n",
    "        # Convert to DataFrame for CSV export\n",
    "        collections = data.get(\"collections\", [])  # Extract collections list\n",
    "        df = pd.DataFrame(collections)\n",
    "\n",
    "        if not df.empty:\n",
    "            # Append to the main DataFrame\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "            # Save data to CSV (append mode)\n",
    "            df.to_csv(csv_filename, mode=\"a\", header=not page_count, index=False)\n",
    "\n",
    "            page_count += 1\n",
    "            \n",
    "\n",
    "        # Extract next token for pagination\n",
    "        next_token = data.get(\"next\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"✅ No more pages to fetch. Process completed.\")\n",
    "            print(f\"✅ Page {page_count} saved.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        print(f\"✅ Page {page_count} saved.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = '/Users/cpysleeper/comp631_proj/data_solana/opensea_collections_solana.csv'\n",
    "output_folder = '/Users/cpysleeper/comp631_proj/data_solana/separated_collections'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "extracted_count =0\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', name)\n",
    "\n",
    "# Read the CSV file and process collections\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    headers = reader.fieldnames\n",
    "\n",
    "    \n",
    "    for row in reader:\n",
    "        description = row.get('description', '').strip()\n",
    "        if description:\n",
    "            filename = sanitize_filename(row.get('collection', 'unknown')) + \".json\"\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(row, out_f, indent=4)\n",
    "            \n",
    "            extracted_count += 1\n",
    "\n",
    "print(f\"Separation complete. Extracted {extracted_count} collections with descriptions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484b873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
