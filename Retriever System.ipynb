{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3399e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from sentence-transformers) (1.11.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./miniconda3/envs/gdal0/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.4/418.4 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.0\n",
      "    Uninstalling transformers-4.35.0:\n",
      "      Successfully uninstalled transformers-4.35.0\n",
      "Successfully installed huggingface-hub-0.29.3 safetensors-0.5.3 sentence-transformers-3.4.1 tokenizers-0.21.1 transformers-4.50.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a2861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data has been extracted to 'solana_collections.csv' with document_id.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory with JSON files\n",
    "folder_path = \"/Users/cpysleeper/comp631_proj/data_solana/separated_collections\"\n",
    "\n",
    "# Initialize a list to hold records\n",
    "records = []\n",
    "\n",
    "# Loop through all JSON files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "                records.append({\n",
    "                    \"title\": data.get(\"name\", \"\"),\n",
    "                    \"text\": data.get(\"description\", \"\")\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"⚠️ Skipping invalid JSON: {filename}\")\n",
    "\n",
    "# Create DataFrame and add document_id as a column\n",
    "df = pd.DataFrame(records)\n",
    "df.insert(0, \"document_id\", range(len(df)))  # Add integer index as a column\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"solana_collections.csv\", index=False)\n",
    "\n",
    "print(\"✅ Data has been extracted to 'solana_collections.csv' with document_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48080fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2569aba9be804648b49dd3c3b1ab7d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d0452d3678446597c6d8788b1a74d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3601c4d0fe43e491d56a417a15e83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e590fe111d44be5a4da2eb77deb029d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_xlm-roberta_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b7226e95f348349b1d18b122313cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1670ffea1b4a50a30b234736cb54c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a7473979d946e99f29f310688cd2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36a4b4f87f341b9b1f6b363ec978279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9335c8f9b34c4ddc8923f9ab56cadae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b445f1348c254928a20dfc2bf78aa695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f64eb8aabf43b385fec53ff41f00cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bcd6bf049249e59329367abf0fb141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Document ID: 20543\n",
      "Title: monies TIX\n",
      "Text: Essence lottery tickets\n",
      "\n",
      "Result 2:\n",
      "Document ID: 31037\n",
      "Title: triggered TIX\n",
      "Text: Essence lottery tickets\n",
      "\n",
      "Result 3:\n",
      "Document ID: 15551\n",
      "Title: zombie lottery\n",
      "Text: biggest lottery\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load your NFT collection CSV\n",
    "df = pd.read_csv(\"solana_collections.csv\")\n",
    "\n",
    "# Combine title and text for better embeddings\n",
    "documents = (df[\"title\"].fillna('') + \" \" + df[\"text\"].fillna('')).tolist()\n",
    "\n",
    "# Load the multilingual E5 model\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "\n",
    "# Add instruction prefix for embedding queries and documents (E5-specific)\n",
    "doc_embeddings = model.encode(\n",
    "    [f\"passage: {doc}\" for doc in documents],\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Map index to original document IDs\n",
    "id_map = df[[\"document_id\", \"title\", \"text\"]].reset_index(drop=True)\n",
    "\n",
    "# --- SEARCH FUNCTION ---\n",
    "def search_nft(query: str, k: int = 5):\n",
    "    query_embed = model.encode(f\"query: {query}\", convert_to_numpy=True)\n",
    "    query_embed = np.expand_dims(query_embed, axis=0)\n",
    "    \n",
    "    distances, indices = index.search(query_embed, k)\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = id_map.iloc[idx]\n",
    "        results.append({\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"text\": result[\"text\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Example Usage ---\n",
    "query = \"lottery ticket with unique rewards\"\n",
    "results = search_nft(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Document ID: {doc['document_id']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982449ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Document ID: 33812\n",
      "Title: Anime Art\n",
      "Text: A collection of anime art\n",
      "JPG and Animation\n",
      "\n",
      "Result 2:\n",
      "Document ID: 25554\n",
      "Title: Gamer\n",
      "Text: Game fantastics\n",
      "\n",
      "Result 3:\n",
      "Document ID: 25563\n",
      "Title: Childhood Game\n",
      "Text: Collections of animated pixel art, based on childhood game.\n",
      "\n",
      "Result 4:\n",
      "Document ID: 24993\n",
      "Title: Genjitsu in Pixels\n",
      "Text: Visions from my mind, thoughts, brought to life in pixels on a screen...\n",
      "\n",
      "Result 5:\n",
      "Document ID: 24013\n",
      "Title: SC art space\n",
      "Text: 1/1 cartoon 2D art\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"gaming or animation\"\n",
    "results = search_nft(query, k=5)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Document ID: {doc['document_id']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fcbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01b6781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9813f534f39a4a30bc794a1a80de6853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Encode documents using MPS backend\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m doc_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassage: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdoc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# critical for MPS support\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create FAISS index (CPU only)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m dimension \u001b[38;5;241m=\u001b[39m doc_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gdal0/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:652\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 652\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    656\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set device: MPS (Apple GPU) or CPU fallback\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# Load your NFT CSV\n",
    "df = pd.read_csv(\"solana_collections.csv\")\n",
    "documents = (df[\"title\"].fillna('') + \" \" + df[\"text\"].fillna('')).tolist()\n",
    "\n",
    "# Load the E5 embedding model with MPS support\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "model.to(device)\n",
    "\n",
    "# Encode documents using MPS backend\n",
    "doc_embeddings = model.encode(\n",
    "    [f\"passage: {doc}\" for doc in documents],\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    device=device  # critical for MPS support\n",
    ")\n",
    "\n",
    "# Create FAISS index (CPU only)\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Map index to original documents\n",
    "id_map = df[[\"document_id\", \"title\", \"text\"]].reset_index(drop=True)\n",
    "\n",
    "# --- Search Function ---\n",
    "def search_nft(query: str, k: int = 5):\n",
    "    query_embed = model.encode(\n",
    "        f\"query: {query}\",\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "    query_embed = np.expand_dims(query_embed, axis=0)\n",
    "    \n",
    "    distances, indices = index.search(query_embed, k)\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = id_map.iloc[idx]\n",
    "        results.append({\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"text\": result[\"text\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Example Usage ---\n",
    "query = \"lottery ticket with essence rewards\"\n",
    "results = search_nft(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n🔎 Result {i}:\")\n",
    "    print(f\"ID: {doc['document_id']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal0",
   "language": "python",
   "name": "gdal0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
