{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory with JSON files\n",
    "folder_path = \"/Users/cpysleeper/comp631_proj/data_solana/separated_collections\"\n",
    "\n",
    "# Initialize a list to hold records\n",
    "records = []\n",
    "\n",
    "# Loop through all JSON files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "                records.append({\n",
    "                    \"title\": data.get(\"name\", \"\"),\n",
    "                    \"text\": data.get(\"description\", \"\")\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid JSON: {filename}\")\n",
    "\n",
    "# Create DataFrame and add document_id as a column\n",
    "df = pd.DataFrame(records)\n",
    "df.insert(0, \"document_id\", range(len(df)))  # Add integer index as a column\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"NFT_collections.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Data has been extracted to 'NFT_collections.csv' with document_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ecc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load your NFT collection CSV\n",
    "df = pd.read_csv(\"solana_collections.csv\")\n",
    "\n",
    "# Combine title and text for better embeddings\n",
    "documents = (df[\"title\"].fillna('') + \" \" + df[\"text\"].fillna('')).tolist()\n",
    "\n",
    "# Load the multilingual E5 model\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "\n",
    "# Add instruction prefix for embedding queries and documents (E5-specific)\n",
    "doc_embeddings = model.encode(\n",
    "    [f\"passage: {doc}\" for doc in documents],\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Map index to original document IDs\n",
    "id_map = df[[\"document_id\", \"title\", \"text\"]].reset_index(drop=True)\n",
    "\n",
    "# --- SEARCH FUNCTION ---\n",
    "def search_nft(query: str, k: int = 5):\n",
    "    query_embed = model.encode(f\"query: {query}\", convert_to_numpy=True)\n",
    "    query_embed = np.expand_dims(query_embed, axis=0)\n",
    "    \n",
    "    distances, indices = index.search(query_embed, k)\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = id_map.iloc[idx]\n",
    "        results.append({\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"text\": result[\"text\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Example Usage ---\n",
    "query = \"lottery ticket with unique rewards\"\n",
    "results = search_nft(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Document ID: {doc['document_id']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfeec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"gaming or animation\"\n",
    "results = search_nft(query, k=5)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Document ID: {doc['document_id']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204082c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbe727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set device: MPS (Apple GPU) or CPU fallback\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# Load your NFT CSV\n",
    "df = pd.read_csv(\"solana_collections.csv\")\n",
    "documents = (df[\"title\"].fillna('') + \" \" + df[\"text\"].fillna('')).tolist()\n",
    "\n",
    "# Load the E5 embedding model with MPS support\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "model.to(device)\n",
    "\n",
    "# Encode documents using MPS backend\n",
    "doc_embeddings = model.encode(\n",
    "    [f\"passage: {doc}\" for doc in documents],\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    device=device  # critical for MPS support\n",
    ")\n",
    "\n",
    "# Create FAISS index (CPU only)\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Map index to original documents\n",
    "id_map = df[[\"document_id\", \"title\", \"text\"]].reset_index(drop=True)\n",
    "\n",
    "# --- Search Function ---\n",
    "def search_nft(query: str, k: int = 5):\n",
    "    query_embed = model.encode(\n",
    "        f\"query: {query}\",\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "    query_embed = np.expand_dims(query_embed, axis=0)\n",
    "    \n",
    "    distances, indices = index.search(query_embed, k)\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = id_map.iloc[idx]\n",
    "        results.append({\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"text\": result[\"text\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Example Usage ---\n",
    "query = \"lottery ticket with essence rewards\"\n",
    "results = search_nft(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nüîé Result {i}:\")\n",
    "    print(f\"ID: {doc['document_id']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal0",
   "language": "python",
   "name": "gdal0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
